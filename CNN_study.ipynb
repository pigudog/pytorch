{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建卷积神经网络\n",
    "- 卷积神经网络的输入层与传统神经网络有些区别，需要重新设计，但是训练模块是基本一致的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets,transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 首先读取数据\n",
    "- 分别构建训练接和检测集（验证集）\n",
    "- DataLoader来迭代取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "input_size = 28 # 图像的总尺寸是28*28\n",
    "num_classes = 10\n",
    "num_epochs = 3\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集\n",
    "train_dataset = datasets.MNIST(root='./data',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "# 测试集\n",
    "test_dataset = datasets.MNIST(root='./data',\n",
    "                               transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建batch数据\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积网络模块构建\n",
    "- 一般卷积层，relu层是一起的，两次卷积一次池化\n",
    "- 注意卷积最后结果还是一个特征图，需要把图转化为向量再做分类或者回归任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # 输入大小为（1,28,28）\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,          # 灰度图\n",
    "                out_channels=16,        # 要得到多少个特征图\n",
    "                kernel_size=5,          # 卷积核大小\n",
    "                stride=1,               # 步长\n",
    "                padding=2               # 如果希望卷积后的特征图的大小跟原来一样,需要设置padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                          # 此时输出的特征图为（16，28，28） \n",
    "            nn.ReLU(),                  # ReLU层\n",
    "            nn.MaxPool2d(kernel_size=2),# 进行池化操作（2x2），输出结果为(16,14,14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(     # 下个一个卷积输入为（16，14，14）\n",
    "            nn.Conv2d(16,32,5,1,2),     # 输出（32，14，14）\n",
    "            nn.ReLU(),                  # ReLU层\n",
    "            nn.MaxPool2d(2)             # 输出（32，7，7）\n",
    "        )\n",
    "        self.out = nn.Linear(32*7*7,10) # 全连接层得到的结果\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0),-1)        # flatten操作，结果是(batch_size,32x7x7)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这将准去率作为最终的评估标准\n",
    "def accuracy(predictions,labels):\n",
    "    pred = torch.max(predictions.data,1)[1]\n",
    "    rights = pred.eq(labels.data.view_as(pred)).sum()\n",
    "    return rights,len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前epoch:0 [0/60000 (0%)]\t损失:2.319847\t训练集准确率:9.38%\t检测集准确率:9.75%\n",
      "当前epoch:0 [6400/60000 (11%)]\t损失:0.046195\t训练集准确率:84.75%\t检测集准确率:94.95%\n",
      "当前epoch:0 [12800/60000 (21%)]\t损失:0.069349\t训练集准确率:90.33%\t检测集准确率:97.08%\n",
      "当前epoch:0 [19200/60000 (32%)]\t损失:0.057271\t训练集准确率:92.42%\t检测集准确率:96.85%\n",
      "当前epoch:0 [25600/60000 (43%)]\t损失:0.007357\t训练集准确率:93.55%\t检测集准确率:97.61%\n",
      "当前epoch:0 [32000/60000 (53%)]\t损失:0.029495\t训练集准确率:94.38%\t检测集准确率:97.92%\n",
      "当前epoch:0 [38400/60000 (64%)]\t损失:0.109703\t训练集准确率:94.91%\t检测集准确率:97.99%\n",
      "当前epoch:0 [44800/60000 (75%)]\t损失:0.002581\t训练集准确率:95.35%\t检测集准确率:98.21%\n",
      "当前epoch:0 [51200/60000 (85%)]\t损失:0.113456\t训练集准确率:95.66%\t检测集准确率:98.14%\n",
      "当前epoch:0 [57600/60000 (96%)]\t损失:0.268721\t训练集准确率:95.92%\t检测集准确率:98.48%\n",
      "当前epoch:1 [0/60000 (0%)]\t损失:0.065312\t训练集准确率:96.88%\t检测集准确率:98.50%\n",
      "当前epoch:1 [6400/60000 (11%)]\t损失:0.006033\t训练集准确率:98.25%\t检测集准确率:98.22%\n",
      "当前epoch:1 [12800/60000 (21%)]\t损失:0.044511\t训练集准确率:98.22%\t检测集准确率:98.32%\n",
      "当前epoch:1 [19200/60000 (32%)]\t损失:0.241321\t训练集准确率:98.15%\t检测集准确率:97.96%\n",
      "当前epoch:1 [25600/60000 (43%)]\t损失:0.004480\t训练集准确率:98.17%\t检测集准确率:98.81%\n",
      "当前epoch:1 [32000/60000 (53%)]\t损失:0.063222\t训练集准确率:98.15%\t检测集准确率:98.52%\n",
      "当前epoch:1 [38400/60000 (64%)]\t损失:0.043988\t训练集准确率:98.15%\t检测集准确率:98.21%\n",
      "当前epoch:1 [44800/60000 (75%)]\t损失:0.270496\t训练集准确率:98.10%\t检测集准确率:98.28%\n",
      "当前epoch:1 [51200/60000 (85%)]\t损失:0.155426\t训练集准确率:98.11%\t检测集准确率:98.51%\n",
      "当前epoch:1 [57600/60000 (96%)]\t损失:0.067508\t训练集准确率:98.11%\t检测集准确率:98.24%\n",
      "当前epoch:2 [0/60000 (0%)]\t损失:0.020413\t训练集准确率:100.00%\t检测集准确率:98.14%\n",
      "当前epoch:2 [6400/60000 (11%)]\t损失:0.106539\t训练集准确率:98.31%\t检测集准确率:98.88%\n",
      "当前epoch:2 [12800/60000 (21%)]\t损失:0.003671\t训练集准确率:98.36%\t检测集准确率:98.76%\n",
      "当前epoch:2 [19200/60000 (32%)]\t损失:0.007958\t训练集准确率:98.40%\t检测集准确率:98.48%\n",
      "当前epoch:2 [25600/60000 (43%)]\t损失:0.140403\t训练集准确率:98.44%\t检测集准确率:98.73%\n",
      "当前epoch:2 [32000/60000 (53%)]\t损失:0.005187\t训练集准确率:98.39%\t检测集准确率:98.89%\n",
      "当前epoch:2 [38400/60000 (64%)]\t损失:0.009882\t训练集准确率:98.38%\t检测集准确率:98.49%\n",
      "当前epoch:2 [44800/60000 (75%)]\t损失:0.092125\t训练集准确率:98.32%\t检测集准确率:98.67%\n",
      "当前epoch:2 [51200/60000 (85%)]\t损失:0.212251\t训练集准确率:98.29%\t检测集准确率:98.26%\n",
      "当前epoch:2 [57600/60000 (96%)]\t损失:0.002924\t训练集准确率:98.31%\t检测集准确率:98.63%\n"
     ]
    }
   ],
   "source": [
    "# 训练网络模型\n",
    "# 实例化\n",
    "net = CNN()\n",
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 优化器\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.01) # 定义优化器\n",
    "\n",
    "# 开始训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    train_rights = []\n",
    "    for batch_idx,(data,target) in enumerate(train_loader): # 针对容器中的每一个批进行循环\n",
    "        net.train()\n",
    "        output = net(data)\n",
    "        loss = criterion(output,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        right = accuracy(output,target)\n",
    "        train_rights.append(right)\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            net.eval()\n",
    "            val_rights = []\n",
    "            for (data,target) in test_loader:\n",
    "                output = net(data)\n",
    "                right = accuracy(output,target)\n",
    "                val_rights.append(right)\n",
    "                \n",
    "            # 准确率计算\n",
    "            train_r = (sum([tup[0] for tup in train_rights]),sum([tup[1] for tup in train_rights]))\n",
    "            val_r = (sum([tup[0] for tup in val_rights]),sum([tup[1] for tup in val_rights]))\n",
    "            \n",
    "            print(\"当前epoch:{} [{}/{} ({:.0f}%)]\\t损失:{:6f}\\t训练集准确率:{:.2f}%\\t检测集准确率:{:.2f}%\".format(\n",
    "                epoch,batch_idx*batch_size,len(train_loader.dataset),\n",
    "                100.*batch_idx/len(train_loader),\n",
    "                loss.data,\n",
    "                100.*train_r[0].numpy()/train_r[1],\n",
    "                100.*val_r[0]/val_r[1]\n",
    "            ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\program\\pytorch\\CNN_study.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/program/pytorch/CNN_study.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 训练网络模型\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/program/pytorch/CNN_study.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# 实例化\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/program/pytorch/CNN_study.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m net \u001b[39m=\u001b[39m CNN()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/program/pytorch/CNN_study.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m net\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/program/pytorch/CNN_study.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# 损失函数\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/program/pytorch/CNN_study.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[1;32me:\\download\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32me:\\download\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32me:\\download\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32me:\\download\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32me:\\download\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[1;32me:\\download\\python\\lib\\site-packages\\torch\\cuda\\__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# 训练网络模型\n",
    "# 实例化\n",
    "net = CNN()\n",
    "net.to(\"cuda\")\n",
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 优化器\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.01) # 定义优化器\n",
    "\n",
    "# 开始训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    train_rights = []\n",
    "    for batch_idx,(data,target) in enumerate(train_loader): # 针对容器中的每一个批进行循环\n",
    "        net.train()\n",
    "        output = net(data)\n",
    "        loss = criterion(output,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        right = accuracy(output,target)\n",
    "        train_rights.append(right)\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            net.eval()\n",
    "            val_rights = []\n",
    "            for (data,target) in test_loader:\n",
    "                output = net(data)\n",
    "                right = accuracy(output,target)\n",
    "                val_rights.append(right)\n",
    "                \n",
    "            # 准确率计算\n",
    "            train_r = (sum([tup[0] for tup in train_rights]),sum([tup[1] for tup in train_rights]))\n",
    "            val_r = (sum([tup[0] for tup in val_rights]),sum([tup[1] for tup in val_rights]))\n",
    "            \n",
    "            print(\"当前epoch:{} [{}/{} ({:.0f}%)]\\t损失:{:6f}\\t训练集准确率:{:.2f}%\\t检测集准确率:{:.2f}%\".format(\n",
    "                epoch,batch_idx*batch_size,len(train_loader.dataset),\n",
    "                100.*batch_idx/len(train_loader),\n",
    "                loss.data,\n",
    "                100.*train_r[0].numpy()/train_r[1],\n",
    "                100.*val_r[0]/val_r[1]\n",
    "            ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e12f081f3b69c47b407a411e35ba4814a2bf694e6a041dab3adb1673ea66113c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
