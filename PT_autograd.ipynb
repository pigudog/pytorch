{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 框架真正厉害的点？\n",
    "可以帮我们把反向传播全部计算好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0264, -0.3336,  1.2340,  0.7088],\n",
       "        [-0.6084,  0.9388, -0.4922,  0.0050],\n",
       "        [-0.0308, -0.3945, -1.1966,  0.0087]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1 - recommended\n",
    "x = torch.randn(3,4,requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3705, -0.2516,  1.0966, -0.6605],\n",
       "        [ 0.9128,  0.8298,  1.6777,  0.6767],\n",
       "        [ 0.2689,  0.1837,  0.6901,  0.9913]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2\n",
    "x = torch.randn(3,4)\n",
    "x.requires_grad = True\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6648, -0.7245, -0.5792,  0.6969],\n",
       "        [-0.9014,  0.7183, -1.4697,  1.0863],\n",
       "        [ 0.8653,  0.1859,  0.4625,  0.0201]], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randn(3,4,requires_grad=True)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2943, -0.9761,  0.5174,  0.0364],\n",
       "        [ 0.0114,  1.5481,  0.2080,  1.7631],\n",
       "        [ 1.1342,  0.3695,  1.1526,  1.0114]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = x+b\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4817, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = t.sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4817, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad\n",
    "## 这儿的变量就是b，一元一次函数求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然没有指定t的required_grad但是需要用到它，也会默认的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad,b.requires_grad,t.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate flow - example \n",
    "x = torch.rand(1)\n",
    "y = torch.rand(1)\n",
    "w = torch.rand(1,requires_grad=True)\n",
    "y = w*x\n",
    "z = y+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check autograde\n",
    "x.requires_grad,b.requires_grad,w.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## again\n",
    "x = torch.rand(1)\n",
    "b = torch.rand(1,requires_grad=True)\n",
    "w = torch.rand(1,requires_grad=True)\n",
    "y = w*x\n",
    "z = y+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True, True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check autograde\n",
    "x.requires_grad,b.requires_grad,w.requires_grad,y.requires_grad # y is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, False, False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_leaf,w.is_leaf,b.is_leaf,y.is_leaf,z.is_leaf\n",
    "# z = wx+b,是否为叶子结点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back propagation\n",
    "z.backward(retain_graph=True) # 注意，如果不清空梯度会累加起来\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6417])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归试水"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造一组输入数据X与对应的标签Y\n",
    "import numpy as np\n",
    "x_values = [i for i in range(11)]\n",
    "x_train  = np.array(x_values,dtype = np.float64)\n",
    "x_train = x_train.reshape(-1,1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_values = [2*i+1 for i in range(11)]\n",
    "y_train = np.array(y_values,dtype=np.float64)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性回归就是不添加激活函数的全连接层\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(LinearRegressionModel,self).__init__()\n",
    "        self.linear = nn.Linear(input_dim,output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim,output_dim,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 制定好参数和损失函数\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "# 一般分类任务是交叉熵，回归任务mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50,loss 1.6038194416978513e-06\n",
      "epoch 100,loss 9.14937288598594e-07\n",
      "epoch 150,loss 5.220479692980007e-07\n",
      "epoch 200,loss 2.977829467454285e-07\n",
      "epoch 250,loss 1.698910097047701e-07\n",
      "epoch 300,loss 9.69701261510636e-08\n",
      "epoch 350,loss 5.5252801445249133e-08\n",
      "epoch 400,loss 3.152409888684815e-08\n",
      "epoch 450,loss 1.7955974485062143e-08\n",
      "epoch 500,loss 1.0293295105157085e-08\n",
      "epoch 550,loss 5.8536993030600115e-09\n",
      "epoch 600,loss 3.361364520060306e-09\n",
      "epoch 650,loss 1.920833492263796e-09\n",
      "epoch 700,loss 1.0967059482780428e-09\n",
      "epoch 750,loss 6.302759425480531e-10\n",
      "epoch 800,loss 3.575153784129981e-10\n",
      "epoch 850,loss 2.1995431864763049e-10\n",
      "epoch 900,loss 1.3579505575567197e-10\n",
      "epoch 950,loss 7.115891903497484e-11\n",
      "epoch 1000,loss 3.915478011262685e-11\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    epoch+=1\n",
    "    # 注意转为tensor\n",
    "    inputs = torch.tensor(x_train)\n",
    "    labels = torch.tensor(y_train)\n",
    "    \n",
    "    # 梯度要清零每一次迭代\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 前向传播\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = criterion(outputs,labels)\n",
    "    \n",
    "    # 返回传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新权重\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(\"epoch {},loss {}\".format(epoch,loss.item()))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "537b928a382ecc1da1b2ff4d48d3aa27b392cceaea5676c08c31c2fea16f125b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
